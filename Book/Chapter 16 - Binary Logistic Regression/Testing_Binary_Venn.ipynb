{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Neural_Network_Class as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from venn_data import create_venn_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\":  (0.12 , 0.12, 0.12, 1),\n",
    "    \"axes.facecolor\": (0.12 , 0.12, 0.12, 1),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type \n",
    "dense1: nn.Layer_Dense \n",
    "activation1: nn.Activation_ReLU\n",
    "dense2: nn.Layer_Dense\n",
    "output_activation: nn.Activation_Sigmoid\n",
    "loss_function: nn.Loss_BinaryCrossentropy\n",
    "optimizer: nn.Optimizer_Adam\n",
    "\n",
    "x1_span: np.ndarray\n",
    "x2_span: np.ndarray\n",
    "xx1: np.ndarray\n",
    "xx2: np.ndarray\n",
    "grid: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(new_X) -> np.ndarray:\n",
    "    dense1.forward(new_X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    output_activation.forward(dense2.output)\n",
    "    predictions = (output_activation.output > 0.5) * 1  # Boolean to int\n",
    "    return predictions\n",
    "\n",
    "def distribution(new_X) -> np.ndarray:\n",
    "    dense1.forward(new_X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    output_activation.forward(dense2.output)\n",
    "    return output_activation.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z_map() -> np.ndarray:\n",
    "    Z = predict(grid)\n",
    "    Z[:, 0] = Z[:, 0] * 2\n",
    "    Z[:, 1] = Z[:, 1] * 4\n",
    "    Z[:, 2] = Z[:, 2] * 8\n",
    "    Z = np.sum(Z, axis=1)\n",
    "    return Z.reshape(xx1.shape)\n",
    "\n",
    "def calc_distribs_z_map() -> tuple[np.ndarray]:\n",
    "    Z = distribution(grid)\n",
    "    Z1 = Z[:, 0].reshape(xx1.shape)\n",
    "    Z2 = Z[:, 1].reshape(xx1.shape)\n",
    "    Z3 = Z[:, 2].reshape(xx1.shape)\n",
    "    return Z1, Z2, Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, epochs=10001, Z_map_frames=False):\n",
    "    training_history = {'acc': [], 'loss': [], 'data_loss': [], \"regularization_loss\": [], \"LR\": [],\n",
    "                        \"Z_map\": [], \"distrib_output\": [], \"Z_map_distrib\": []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Forward pass\n",
    "        predictions = predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        \n",
    "        # Loss calculation\n",
    "        data_loss = loss_function.calculate(output_activation.output, y)\n",
    "        regularization_loss = loss_function.regularization_loss(dense1) + \\\n",
    "                              loss_function.regularization_loss(dense2)\n",
    "        loss = data_loss + regularization_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(output_activation.output, y)\n",
    "        output_activation.backward(loss_function.dinputs)\n",
    "        dense2.backward(output_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        # Update training history\n",
    "        training_history['acc'].append(accuracy)\n",
    "        training_history['loss'].append(loss)\n",
    "        training_history['data_loss'].append(data_loss)\n",
    "        training_history['regularization_loss'].append(regularization_loss)\n",
    "        training_history['distrib_output'].append(output_activation.output)\n",
    "        training_history['LR'].append(optimizer.current_learning_rate)\n",
    "\n",
    "    training_history['Z_map'].append(calc_z_map())\n",
    "    \n",
    "    return training_history, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Layer\n",
    "dense1 = nn.Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "activation1 = nn.Activation_ReLU()\n",
    "dense2 = nn.Layer_Dense(64, 3)\n",
    "output_activation = nn.Activation_Sigmoid()\n",
    "loss_function = nn.Loss_BinaryCrossentropy()\n",
    "optimizer = nn.Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# Data\n",
    "X, y = create_venn_data(samples=1000, classes=3)\n",
    "\n",
    "x1_span = np.linspace(min(X[:, 0]) - 0.1, max(X[:, 0]) + 0.1, 500) # 300 is nice\n",
    "x2_span = np.linspace(min(X[:, 1]) - 0.1, max(X[:, 1]) + 0.1, 500) # 300 is nice\n",
    "xx1, xx2 = np.meshgrid(x1_span, x2_span)\n",
    "grid = np.vstack((xx1.ravel(), xx2.ravel())).T # Concatenation des deux matrices ligne par ligne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history, predictions = train(X, y, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loss = \", training_history['loss'][-1])\n",
    "print(\"Training Accuracy = \", training_history['acc'][-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_countour_graph(X, y, predictions, Z):\n",
    "    # Fix Array\n",
    "    fix_predictions = predictions.copy()\n",
    "    fix_predictions[:,0] *= 2\n",
    "    fix_predictions[:,1] *= 4\n",
    "    fix_predictions[:,2] *= 8\n",
    "    fix_y = y.copy()\n",
    "    fix_y[:,0] *= 2\n",
    "    fix_y[:,1] *= 4\n",
    "    fix_y[:,2] *= 8\n",
    "    # Plot frontiere de d√©cision\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(xx1, xx2, Z, cmap='coolwarm', alpha=0.5)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.sum(fix_y, axis=1, keepdims=True), cmap='coolwarm')\n",
    "    plt.title('Decision boundary of y_true')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.contourf(xx1, xx2, Z, cmap='coolwarm', alpha=0.5)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.sum(fix_predictions, axis=1, keepdims=True), cmap='coolwarm')\n",
    "    plt.title('Decision boundary of y_pred')\n",
    "\n",
    "def plot_training_graph(training_history):\n",
    "    # Plot courbe d'apprentissage\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(training_history[\"loss\"], label='train loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(training_history[\"data_loss\"], label='train data loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(training_history[\"regularization_loss\"], label='train regularization loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(training_history[\"acc\"], label='train acc')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(training_history[\"LR\"], label='Learning rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def animate_coutour(Z):\n",
    "    ax.contourf(xx1, xx2, Z, cmap='coolwarm', alpha=0.5, zorder=-1)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap='coolwarm', zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_graph(training_history)\n",
    "plot_countour_graph(X, y, predictions, training_history['Z_map'][-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_predictions = predictions.copy()\n",
    "fix_predictions[:,0] *= 2\n",
    "fix_predictions[:,1] *= 4\n",
    "fix_predictions[:,2] *= 8\n",
    "fix_y = y.copy()\n",
    "fix_y[:,0] *= 2\n",
    "fix_y[:,1] *= 4\n",
    "fix_y[:,2] *= 8\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter3d( \n",
    "        x=xx1.flatten(),\n",
    "        y=xx2.flatten(),\n",
    "        z=training_history['Z_map'][-1].flatten(),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=training_history['Z_map'][-1].flatten(),                \n",
    "            colorscale=['blue', 'white', 'red'],  \n",
    "            opacity=0.2,\n",
    "            reversescale=False\n",
    "        )\n",
    "    ),\n",
    "    go.Scatter3d( \n",
    "        x=X[:, 0].flatten(),\n",
    "        y=X[:, 1].flatten(),\n",
    "        z=np.sum(fix_predictions, axis=1),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=np.sum(fix_y, axis=1),                \n",
    "            colorscale=['blue', 'white', 'red'],  \n",
    "            opacity=0.8,\n",
    "            reversescale=False\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(template= \"plotly_dark\", margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.layout.scene.camera.projection.type = \"orthographic\"\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_output_map = calc_distribs_z_map()\n",
    "distrib_output_X = training_history['distrib_output'][-1].T\n",
    "distrib_class_1 = distrib_output_map[0]\n",
    "distrib_class_2 = distrib_output_map[1]\n",
    "distrib_class_3 = distrib_output_map[2]\n",
    "\n",
    "calc_distribs_z_map()\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Surface(\n",
    "        x=xx1,\n",
    "        y=xx2,\n",
    "        z=distrib_class_1,\n",
    "        colorscale='Blues',\n",
    "        opacity=1,\n",
    "    ),\n",
    "    go.Surface(\n",
    "        x=xx1,\n",
    "        y=xx2,\n",
    "        z=distrib_class_2,\n",
    "        colorscale='Greys',\n",
    "        opacity=1,\n",
    "    ),\n",
    "    go.Surface(\n",
    "        x=xx1,\n",
    "        y=xx2,\n",
    "        z=distrib_class_3,\n",
    "        colorscale='Reds',\n",
    "        opacity=1,\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(template= \"plotly_dark\", margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.layout.scene.camera.projection.type = \"orthographic\"\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=['Surface 1', 'Surface 2', 'Surface 3'], specs=[[{'type': 'surface'}, {'type': 'surface'}, {'type': 'surface'}]])\n",
    "\n",
    "distrib_output_map = calc_distribs_z_map()\n",
    "distrib_output_X = training_history['distrib_output'][-1].T\n",
    "distrib_class_1 = distrib_output_map[0]\n",
    "distrib_class_2 = distrib_output_map[1]\n",
    "distrib_class_3 = distrib_output_map[2]\n",
    "\n",
    "fig.add_trace(go.Surface(x=xx1, y=xx2, z=distrib_class_1, colorscale='Blues', opacity=1), row=1, col=1)\n",
    "fig.add_trace(go.Surface(x=xx1, y=xx2, z=distrib_class_2, colorscale='Greys', opacity=1), row=1, col=2)\n",
    "fig.add_trace(go.Surface(x=xx1, y=xx2, z=distrib_class_3, colorscale='Reds', opacity=1), row=1, col=3)\n",
    "\n",
    "fig.update_layout(template=\"plotly_dark\", margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.layout.scene.camera.projection.type = \"orthographic\"\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y):\n",
    "    test_history = {'acc': 0.0, 'loss':[]}\n",
    "    \n",
    "    predictions = predict(X)\n",
    "\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    loss = loss_function.calculate(output_activation.output, y)\n",
    "\n",
    "    print(f'accuracy: {accuracy:.3f}, ' +\n",
    "            f'loss: {loss:.3f}, ')\n",
    "    \n",
    "    test_history['acc'] = accuracy\n",
    "    test_history['loss'] = loss\n",
    "    \n",
    "    return test_history, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "X_test, y_test = create_venn_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "test_history, predictions_test = test(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training loss differ from test performance by over ~10%, its a common sign of serious overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loss = \", training_history['loss'][-1])\n",
    "print(\"Test Loss = \", test_history['loss'])\n",
    "\n",
    "print(\"Training Accuracy = \", training_history['acc'][-1])\n",
    "print(\"Test Accuracy = \", test_history['acc'])\n",
    "\n",
    "# Compare the training loss and test loss\n",
    "print(f\"{abs(training_history['loss'][-1] - test_history['loss']) / training_history['loss'][-1] * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countour_graph(X_test, y_test, predictions_test, calc_z_map())\n",
    "plot_countour_graph(X, y, predictions, calc_z_map())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPA678",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
